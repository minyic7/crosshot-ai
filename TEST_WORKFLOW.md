# æµ‹è¯•å·¥ä½œæµç¨‹

å®Œæ•´æµ‹è¯•ä»éƒ¨ç½²åˆ° CI/CD è‡ªåŠ¨æ›´æ–°çš„æµç¨‹ã€‚

## å‰ç½®å‡†å¤‡

1. **è®¾ç½® GitHub ç¯å¢ƒå˜é‡**
   ```bash
   # åˆ›å»º .env æ–‡ä»¶ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
   echo "GITHUB_USERNAME=ä½ çš„GitHubç”¨æˆ·å" > .env
   ```

2. **ç¡®ä¿ GitHub Token å·²é…ç½®**
   ```bash
   # ç™»å½• GitHub Container Registry
   echo $GITHUB_TOKEN | docker login ghcr.io -u ä½ çš„GitHubç”¨æˆ·å --password-stdin
   ```

---

## é˜¶æ®µ 1: æœ¬åœ°æ„å»ºæµ‹è¯•

### 1.1 æ„å»º Crawler é•œåƒ

```bash
# æ„å»ºé•œåƒ
docker build -f apps/crawler/Dockerfile -t crawler:local .

# éªŒè¯é•œåƒ
docker images | grep crawler
```

### 1.2 æœ¬åœ°è¿è¡Œå•ä¸ªå®ä¾‹

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/test logs/test

# è¿è¡Œæµ‹è¯•å®ä¾‹
docker run --rm \
  -e PLATFORM=x \
  -e KEYWORDS="AI,Python,Web3" \
  -e MAX_RESULTS=20 \
  -e INTERVAL=60 \
  -e LOG_LEVEL=INFO \
  -v $(pwd)/data/test:/app/data \
  -v $(pwd)/logs/test:/app/logs \
  --name crawler-test \
  crawler:local
```

**é¢„æœŸè¾“å‡º:**
```
ğŸš€ Crawler service starting...
ğŸ“‹ è¿›ç¨‹ ID: 1
ğŸ·ï¸  å¹³å°: x
ğŸ•·ï¸  [x] çˆ¬è™«å¾ªç¯å¯åŠ¨...
ğŸ“‹ [x] é…ç½®:
   - å¹³å°: x
   - å…³é”®è¯: ['AI', 'Python', 'Web3']
   - æœ€å¤§ç»“æœæ•°: 20
ğŸ” [x] å¼€å§‹çˆ¬å–ï¼Œå…³é”®è¯: ['AI', 'Python', 'Web3']
ğŸ“± [x] å¹³å°: X (Twitter)
â³ [x] è¿›åº¦: 10/20 (50.0%)
âœ… [x] çˆ¬å–å®Œæˆ: 20 ä¸ª post
ğŸ’¾ [x] Mock æ•°æ®å·²ä¿å­˜åˆ°: /app/data/mock_data_20260207_123456.json
ğŸ“Š [x] ç»Ÿè®¡:
   - æ€»äº’åŠ¨: 45,234 ç‚¹èµ, 1,234 è¯„è®º
   - åª’ä½“ç±»å‹: {'image': 12, 'video': 6, 'gif': 2}
```

### 1.3 æµ‹è¯•ä¼˜é›…åœæ­¢

æ‰“å¼€å¦ä¸€ä¸ªç»ˆç«¯:
```bash
# å‘é€åœæ­¢ä¿¡å·
docker stop -t 60 crawler-test

# æˆ–ä½¿ç”¨ Ctrl+C
```

**é¢„æœŸè¾“å‡º:**
```
â¸ï¸  [x] æ”¶åˆ° SIGTERM ä¿¡å·ï¼Œå‡†å¤‡ä¼˜é›…åœæ­¢...
ğŸ’¾ [x] æ­£åœ¨ä¿å­˜è¿›åº¦...
âœ… [x] è¿›åº¦å·²ä¿å­˜åˆ° /app/data/progress_x.json
ğŸ‘‹ [x] Crawler service stopped gracefully
```

### 1.4 éªŒè¯æ•°æ®æ–‡ä»¶

```bash
# æŸ¥çœ‹ä¿å­˜çš„æ•°æ®
ls -lh data/test/
cat data/test/progress_x.json | jq .
cat data/test/mock_data_*.json | jq '.items | length'
```

---

## é˜¶æ®µ 2: Docker Compose å¤šå®ä¾‹æµ‹è¯•

### 2.1 å¯åŠ¨ X å¹³å°ï¼ˆ3ä¸ªå®ä¾‹ï¼‰

```bash
# ä½¿ç”¨å¸®åŠ©è„šæœ¬
./scripts/compose-helper.sh up x

# æˆ–ç›´æ¥ä½¿ç”¨ docker-compose
docker-compose -f docker-compose.yml -f docker-compose.x.yml up -d
```

### 2.2 æŸ¥çœ‹è¿è¡ŒçŠ¶æ€

```bash
# æŸ¥çœ‹å®¹å™¨
docker ps --filter "label=platform=x"

# æŸ¥çœ‹æ—¥å¿—ï¼ˆæ‰€æœ‰å®ä¾‹ï¼‰
./scripts/compose-helper.sh logs x

# æŸ¥çœ‹ç‰¹å®šå®ä¾‹æ—¥å¿—
docker logs -f crawler-x-ai
```

**é¢„æœŸçœ‹åˆ° 3 ä¸ª crawler + 1 ä¸ª watchtower è¿è¡Œ:**
```
CONTAINER ID   IMAGE                          STATUS          NAMES
abc123         ghcr.io/.../crawler:latest     Up 10 seconds   crawler-x-ai
def456         ghcr.io/.../crawler:latest     Up 10 seconds   crawler-x-python
ghi789         ghcr.io/.../crawler:latest     Up 10 seconds   crawler-x-web3
jkl012         containrrr/watchtower:latest   Up 10 seconds   watchtower-x
```

### 2.3 éªŒè¯æ•°æ®åˆ†ç¦»

```bash
# æ¯ä¸ªå®ä¾‹åº”è¯¥æœ‰ç‹¬ç«‹çš„æ•°æ®ç›®å½•
tree -L 3 data/x/
```

**é¢„æœŸç»“æ„:**
```
data/x/
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ mock_data_20260207_123456.json
â”‚   â””â”€â”€ progress_x.json
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ mock_data_20260207_123457.json
â”‚   â””â”€â”€ progress_x.json
â””â”€â”€ web3/
    â”œâ”€â”€ mock_data_20260207_123458.json
    â””â”€â”€ progress_x.json
```

### 2.4 å¯åŠ¨å°çº¢ä¹¦å¹³å°ï¼ˆ4ä¸ªå®ä¾‹ï¼‰

```bash
./scripts/compose-helper.sh up xhs

# éªŒè¯è¿è¡Œ
docker ps --filter "label=platform=xhs"
```

### 2.5 åŒæ—¶è¿è¡Œæ‰€æœ‰å¹³å°

```bash
./scripts/compose-helper.sh up all

# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"
```

---

## é˜¶æ®µ 3: CI/CD æµç¨‹æµ‹è¯•

### 3.1 æäº¤ä»£ç è§¦å‘ CI

```bash
# æŸ¥çœ‹å½“å‰çŠ¶æ€
git status

# æäº¤ mock crawler å®ç°
git add apps/crawler/__main__.py
git commit -m "feat: add mock crawler implementation

- Simulate X/XHS/Douyin platform data
- Generate realistic engagement metrics
- Save JSON data files
- Support graceful shutdown during scraping
- Detailed progress logging

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

# æ¨é€åˆ° GitHubï¼ˆè§¦å‘ CI/CDï¼‰
git push origin main
```

### 3.2 ç›‘æ§ GitHub Actions

1. è®¿é—®: `https://github.com/ä½ çš„ç”¨æˆ·å/crosshot-ai/actions`
2. æŸ¥çœ‹ "Build and Push Crawler Image" workflow
3. ç­‰å¾…æ„å»ºå®Œæˆï¼ˆçº¦ 3-5 åˆ†é’Ÿï¼‰

**é¢„æœŸæ­¥éª¤:**
```
âœ… Checkout code
âœ… Set up Docker Buildx
âœ… Log in to GitHub Container Registry
âœ… Extract metadata
âœ… Build and push Docker image
   - Building apps/crawler/Dockerfile
   - Pushing to ghcr.io/ä½ çš„ç”¨æˆ·å/crawler:latest
   - Pushing to ghcr.io/ä½ çš„ç”¨æˆ·å/crawler:sha-abc123
```

### 3.3 éªŒè¯é•œåƒå·²æ¨é€

```bash
# åœ¨æœ¬åœ°æ‹‰å–æ–°é•œåƒ
docker pull ghcr.io/ä½ çš„GitHubç”¨æˆ·å/crawler:latest

# æŸ¥çœ‹é•œåƒä¿¡æ¯
docker inspect ghcr.io/ä½ çš„GitHubç”¨æˆ·å/crawler:latest | jq '.[0].Created'
```

---

## é˜¶æ®µ 4: Watchtower è‡ªåŠ¨æ›´æ–°æµ‹è¯•

### 4.1 è§‚å¯Ÿ Watchtower æ—¥å¿—

```bash
# X å¹³å° watchtowerï¼ˆæ¯ 5 åˆ†é’Ÿæ£€æŸ¥ï¼‰
docker logs -f watchtower-x

# å°çº¢ä¹¦ watchtowerï¼ˆæ¯ 30 åˆ†é’Ÿæ£€æŸ¥ï¼‰
docker logs -f watchtower-xhs
```

**é¢„æœŸæ—¥å¿—ï¼ˆX å¹³å° 5 åˆ†é’Ÿåï¼‰:**
```
time="2026-02-07T12:00:00Z" level=info msg="Checking for updates"
time="2026-02-07T12:00:01Z" level=info msg="Found new image for crawler-x-ai"
time="2026-02-07T12:00:01Z" level=info msg="Stopping container crawler-x-ai (60s timeout)"
time="2026-02-07T12:00:05Z" level=info msg="Container stopped gracefully"
time="2026-02-07T12:00:06Z" level=info msg="Starting container crawler-x-ai"
time="2026-02-07T12:00:07Z" level=info msg="Update complete for crawler-x-ai"
```

### 4.2 éªŒè¯å®¹å™¨å·²æ›´æ–°

```bash
# æŸ¥çœ‹å®¹å™¨å¯åŠ¨æ—¶é—´ï¼ˆåº”è¯¥æ˜¯æœ€è¿‘æ›´æ–°çš„ï¼‰
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}" | grep crawler-x

# éªŒè¯è¿›åº¦å·²æ¢å¤
cat data/x/ai/progress_x.json | jq .
```

**é¢„æœŸè¾“å‡º:**
```
NAMES           STATUS              IMAGE
crawler-x-ai    Up 2 minutes        ghcr.io/.../crawler:latest
crawler-x-python Up About an hour   ghcr.io/.../crawler:latest  # è¿˜æœªæ›´æ–°
crawler-x-web3   Up About an hour   ghcr.io/.../crawler:latest  # è¿˜æœªæ›´æ–°
```

### 4.3 æ‰‹åŠ¨è§¦å‘æ›´æ–°æµ‹è¯•

å¦‚æœä¸æƒ³ç­‰ 5 åˆ†é’Ÿï¼Œå¯ä»¥æ‰‹åŠ¨è§¦å‘:

```bash
# æ–¹æ³• 1: é‡å¯ watchtowerï¼ˆç«‹å³æ£€æŸ¥ï¼‰
docker restart watchtower-x

# æ–¹æ³• 2: æ‰‹åŠ¨æ‹‰å–å¹¶é‡å¯
docker pull ghcr.io/ä½ çš„GitHubç”¨æˆ·å/crawler:latest
docker restart crawler-x-ai
```

---

## é˜¶æ®µ 5: å¢åŠ åŠŸèƒ½å¹¶æµ‹è¯•å®Œæ•´æµç¨‹

### 5.1 ä¿®æ”¹ä»£ç ï¼ˆä¾‹å¦‚ï¼šå¢åŠ æ–°çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰

ç¼–è¾‘ `apps/crawler/__main__.py`ï¼Œåœ¨ç»Ÿè®¡ä¿¡æ¯ä¸­æ·»åŠ æ–°å­—æ®µ:

```python
logger.info(f"ğŸ“Š [{platform}] ç»Ÿè®¡:")
logger.info(f"   - æ€»äº’åŠ¨: {total_likes:,} ç‚¹èµ, {total_comments:,} è¯„è®º")
logger.info(f"   - åª’ä½“ç±»å‹: {media_types_count}")
logger.info(f"   - å¹³å‡ç‚¹èµ: {total_likes//len(items_scraped) if items_scraped else 0}")  # æ–°å¢
```

### 5.2 æäº¤æ›´æ–°

```bash
git add apps/crawler/__main__.py
git commit -m "feat: add average likes to crawler stats

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
git push origin main
```

### 5.3 å®Œæ•´æµç¨‹éªŒè¯

1. **GitHub Actions å¼€å§‹æ„å»º** (0-30ç§’)
   - è®¿é—® Actions é¡µé¢ç¡®è®¤

2. **æ„å»ºæ–°é•œåƒ** (3-5åˆ†é’Ÿ)
   - ç­‰å¾… workflow å®Œæˆ

3. **Watchtower æ£€æµ‹æ›´æ–°** (æœ€å¤š 5 åˆ†é’Ÿ)
   - è§‚å¯Ÿ watchtower-x æ—¥å¿—

4. **ä¼˜é›…åœæ­¢æ—§å®¹å™¨** (0-60ç§’)
   - è§‚å¯Ÿ crawler æ—¥å¿—ï¼Œç¡®è®¤è¿›åº¦å·²ä¿å­˜

5. **å¯åŠ¨æ–°å®¹å™¨** (5-10ç§’)
   - æ–°å®¹å™¨åŠ è½½ä¹‹å‰çš„è¿›åº¦ç»§ç»­è¿è¡Œ

6. **éªŒè¯æ–°åŠŸèƒ½** (ç«‹å³)
   - æŸ¥çœ‹æ—¥å¿—ï¼Œç¡®è®¤æ–°çš„ç»Ÿè®¡ä¿¡æ¯å‡ºç°

```bash
# ä¸€é”®æŸ¥çœ‹æ•´ä¸ªæµç¨‹
watch -n 5 'docker ps --format "table {{.Names}}\t{{.Status}}" | grep crawler-x && echo "---" && docker logs --tail 5 crawler-x-ai 2>&1 | grep "ç»Ÿè®¡"'
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1: Watchtower ä¸æ›´æ–°

**æ£€æŸ¥:**
```bash
# 1. éªŒè¯ watchtower é…ç½®
docker logs watchtower-x | grep -i "error\|warning"

# 2. æ‰‹åŠ¨æ‹‰å–çœ‹æ˜¯å¦æœ‰æ–°é•œåƒ
docker pull ghcr.io/ä½ çš„GitHubç”¨æˆ·å/crawler:latest

# 3. æ£€æŸ¥æ ‡ç­¾è¿‡æ»¤
docker inspect watchtower-x | jq '.[0].Config.Env' | grep WATCHTOWER
```

**è§£å†³æ–¹æ¡ˆ:**
- ç¡®è®¤ GitHub Actions æˆåŠŸæ¨é€é•œåƒ
- éªŒè¯é•œåƒæ ‡ç­¾åŒ¹é…ï¼ˆlatestï¼‰
- æ£€æŸ¥ GITHUB_USERNAME ç¯å¢ƒå˜é‡

### é—®é¢˜ 2: å®¹å™¨æ— æ³•ä¼˜é›…åœæ­¢

**æ£€æŸ¥:**
```bash
# æŸ¥çœ‹åœæ­¢æ—¶çš„æ—¥å¿—
docker stop -t 60 crawler-x-ai
docker logs crawler-x-ai --tail 50
```

**è§£å†³æ–¹æ¡ˆ:**
- ç¡®è®¤ä¿¡å·å¤„ç†å™¨å·²æ³¨å†Œ
- æ£€æŸ¥æ˜¯å¦æœ‰é˜»å¡æ“ä½œ
- å¢åŠ è¶…æ—¶æ—¶é—´

### é—®é¢˜ 3: è¿›åº¦æ²¡æœ‰æ¢å¤

**æ£€æŸ¥:**
```bash
# éªŒè¯è¿›åº¦æ–‡ä»¶å­˜åœ¨
ls -lh data/x/ai/progress_x.json

# éªŒè¯å·æŒ‚è½½æ­£ç¡®
docker inspect crawler-x-ai | jq '.[0].Mounts'
```

**è§£å†³æ–¹æ¡ˆ:**
- ç¡®è®¤å·è·¯å¾„æ­£ç¡®
- æ£€æŸ¥æ–‡ä»¶æƒé™
- éªŒè¯ JSON æ ¼å¼æ­£ç¡®

---

## æ€§èƒ½ç›‘æ§

### å®æ—¶ç›‘æ§æ‰€æœ‰å®¹å™¨

```bash
# CPU å’Œå†…å­˜ä½¿ç”¨
docker stats

# åªçœ‹ crawler
docker stats $(docker ps --filter "label=platform" -q)
```

### æ—¥å¿—èšåˆæŸ¥çœ‹

```bash
# æ‰€æœ‰ X å¹³å°å®ä¾‹çš„æœ€æ–°æ—¥å¿—
for container in crawler-x-ai crawler-x-python crawler-x-web3; do
  echo "=== $container ==="
  docker logs --tail 5 $container 2>&1 | grep "çˆ¬å–å®Œæˆ\|ç»Ÿè®¡"
  echo
done
```

### æ•°æ®ç»Ÿè®¡

```bash
# ç»Ÿè®¡å·²çˆ¬å–çš„æ•°æ®é‡
find data/ -name "mock_data_*.json" -exec jq -r '.total_items' {} \; | awk '{sum+=$1} END {print "æ€»å…±çˆ¬å–:", sum, "æ¡"}'

# ç»Ÿè®¡æ¯ä¸ªå¹³å°
for platform in x xhs; do
  count=$(find data/$platform -name "mock_data_*.json" -exec jq -r '.total_items' {} \; | awk '{sum+=$1} END {print sum}')
  echo "$platform å¹³å°: $count æ¡"
done
```

---

## æ¸…ç†å‘½ä»¤

```bash
# åœæ­¢æ‰€æœ‰
./scripts/compose-helper.sh down all

# æ¸…ç†æ•°æ®ï¼ˆè°¨æ…ï¼ï¼‰
rm -rf data/x data/xhs logs/

# æ¸…ç†é•œåƒ
docker rmi ghcr.io/ä½ çš„GitHubç”¨æˆ·å/crawler:latest
docker rmi crawler:local

# å®Œå…¨é‡ç½®
docker system prune -a --volumes
```

---

## ä¸‹ä¸€æ­¥

æµ‹è¯•é€šè¿‡åï¼Œå¯ä»¥ï¼š

1. **å®ç°çœŸå®çˆ¬è™«é€»è¾‘**
   - æ›¿æ¢ `crawl_platform()` ä¸­çš„ mock ä»£ç 
   - é›†æˆ Playwright è¿›è¡Œå®é™…æŠ“å–
   - æ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘

2. **æ·»åŠ æ•°æ®åº“å­˜å‚¨**
   - å°† JSON æ•°æ®ä¿å­˜åˆ° PostgreSQL/SQLite
   - å®ç°æ•°æ®å»é‡å’Œå¢é‡æ›´æ–°

3. **éƒ¨ç½²åˆ° NAS**
   - å‚è€ƒ `DEPLOYMENT.md` éƒ¨ç½²åˆ° QNAP NAS
   - é…ç½®è‡ªåŠ¨é‡å¯å’Œç›‘æ§

4. **æ·»åŠ  Web Dashboard**
   - æŸ¥çœ‹çˆ¬å–è¿›åº¦å’Œæ•°æ®ç»Ÿè®¡
   - æ§åˆ¶ crawler å¯åœ

5. **é…ç½®å‘Šè­¦é€šçŸ¥**
   - Watchtower Discord/Slack é€šçŸ¥
   - çˆ¬è™«é”™è¯¯å‘Šè­¦

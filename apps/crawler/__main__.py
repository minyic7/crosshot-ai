"""Crawler service entry point - é€šç”¨çˆ¬è™«ï¼Œæ”¯æŒå¤šå¹³å°é…ç½®."""

import asyncio
import logging
import os
import signal
import sys
from pathlib import Path

# é…ç½®æ—¥å¿—
log_level = os.getenv("LOG_LEVEL", "INFO")
logging.basicConfig(
    level=getattr(logging, log_level),
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)

logger = logging.getLogger(__name__)

# å…¨å±€åœæ­¢æ ‡å¿—
shutdown_event = asyncio.Event()

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
PLATFORM = os.getenv("PLATFORM", "unknown")
KEYWORDS = os.getenv("KEYWORDS", "").split(",")
MAX_RESULTS = int(os.getenv("MAX_RESULTS", "100"))
INTERVAL = int(os.getenv("INTERVAL", "3600"))
ENV = os.getenv("ENV", "development")


def handle_shutdown(signum, frame):
    """å¤„ç†ä¼˜é›…åœæ­¢ä¿¡å·."""
    sig_name = signal.Signals(signum).name
    logger.info(f"â¸ï¸  [{PLATFORM}] æ”¶åˆ° {sig_name} ä¿¡å·ï¼Œå‡†å¤‡ä¼˜é›…åœæ­¢...")
    shutdown_event.set()


async def save_progress(state: dict):
    """ä¿å­˜çˆ¬è™«è¿›åº¦åˆ°ç£ç›˜."""
    logger.info(f"ğŸ’¾ [{PLATFORM}] æ­£åœ¨ä¿å­˜è¿›åº¦...")

    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path("/app/data")
    data_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    import json
    progress_file = data_dir / f"progress_{PLATFORM}.json"
    progress_file.write_text(json.dumps(state, ensure_ascii=False, indent=2))

    logger.info(f"âœ… [{PLATFORM}] è¿›åº¦å·²ä¿å­˜åˆ° {progress_file}")


async def load_progress() -> dict:
    """ä»ç£ç›˜åŠ è½½çˆ¬è™«è¿›åº¦."""
    logger.info(f"ğŸ“‚ [{PLATFORM}] åŠ è½½ä¹‹å‰çš„è¿›åº¦...")

    import json
    progress_file = Path("/app/data") / f"progress_{PLATFORM}.json"

    if progress_file.exists():
        try:
            state = json.loads(progress_file.read_text())
            logger.info(f"âœ… [{PLATFORM}] å·²åŠ è½½è¿›åº¦: {state.get('last_update', 'unknown')}")
            return state
        except Exception as e:
            logger.warning(f"âš ï¸  [{PLATFORM}] åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return {}

    logger.info(f"âœ… [{PLATFORM}] ä»å¤´å¼€å§‹")
    return {}


async def crawl_platform(platform: str, keywords: list[str]) -> dict:
    """
    æ ¹æ®å¹³å°ç±»å‹çˆ¬å–æ•°æ® (Mock å®ç°).

    Args:
        platform: å¹³å°åç§° (x, xhs, douyin ç­‰)
        keywords: çˆ¬å–å…³é”®è¯åˆ—è¡¨

    Returns:
        çˆ¬å–ç»“æœç»Ÿè®¡
    """
    logger.info(f"ğŸ” [{platform}] å¼€å§‹çˆ¬å–ï¼Œå…³é”®è¯: {keywords}")

    import random
    import datetime

    # æ¨¡æ‹Ÿä¸åŒå¹³å°çš„æ•°æ®ç»“æ„
    platform_configs = {
        "x": {
            "name": "X (Twitter)",
            "content_type": "post",
            "avg_engagement": (100, 5000),
            "media_types": ["image", "video", "gif"],
        },
        "xhs": {
            "name": "å°çº¢ä¹¦",
            "content_type": "note",
            "avg_engagement": (500, 10000),
            "media_types": ["image", "video"],
        },
        "douyin": {
            "name": "æŠ–éŸ³",
            "content_type": "video",
            "avg_engagement": (1000, 50000),
            "media_types": ["video"],
        },
    }

    config = platform_configs.get(platform, {
        "name": platform,
        "content_type": "content",
        "avg_engagement": (10, 1000),
        "media_types": ["image"],
    })

    logger.info(f"ğŸ“± [{platform}] å¹³å°: {config['name']}")

    # æ¨¡æ‹Ÿçˆ¬å–è¿‡ç¨‹
    items_scraped = []
    total_items = random.randint(max(5, MAX_RESULTS - 20), MAX_RESULTS)

    for i in range(total_items):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
        if shutdown_event.is_set():
            logger.info(f"âš ï¸  [{platform}] çˆ¬å–è¿‡ç¨‹ä¸­æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå·²çˆ¬å– {len(items_scraped)} é¡¹")
            break

        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.1, 0.5))

        # ç”Ÿæˆ mock æ•°æ®
        keyword = random.choice(keywords) if keywords else "default"
        item = {
            "id": f"{platform}_{i+1}_{random.randint(10000, 99999)}",
            "platform": platform,
            "keyword": keyword,
            "type": config["content_type"],
            "title": f"Mock {config['content_type']} about {keyword} #{i+1}",
            "author": f"user_{random.randint(1000, 9999)}",
            "engagement": {
                "likes": random.randint(*config["avg_engagement"]),
                "comments": random.randint(10, 500),
                "shares": random.randint(5, 200),
            },
            "media": {
                "type": random.choice(config["media_types"]),
                "count": random.randint(1, 9),
            },
            "timestamp": datetime.datetime.now().isoformat(),
        }

        items_scraped.append(item)

        # æ¯10ä¸ªæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0:
            logger.info(f"â³ [{platform}] è¿›åº¦: {i+1}/{total_items} ({(i+1)/total_items*100:.1f}%)")

    # ä¿å­˜ mock æ•°æ®åˆ°æ–‡ä»¶
    data_dir = Path("/app/data")
    data_dir.mkdir(parents=True, exist_ok=True)

    import json
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    data_file = data_dir / f"mock_data_{timestamp}.json"

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump({
            "platform": platform,
            "keywords": keywords,
            "scraped_at": datetime.datetime.now().isoformat(),
            "total_items": len(items_scraped),
            "items": items_scraped,
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ’¾ [{platform}] Mock æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
    logger.info(f"âœ… [{platform}] çˆ¬å–å®Œæˆ: {len(items_scraped)} ä¸ª {config['content_type']}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_likes = sum(item["engagement"]["likes"] for item in items_scraped)
    total_comments = sum(item["engagement"]["comments"] for item in items_scraped)
    media_types_count = {}
    for item in items_scraped:
        media_type = item["media"]["type"]
        media_types_count[media_type] = media_types_count.get(media_type, 0) + 1

    # æ–°åŠŸèƒ½ï¼šè®¡ç®—å¹³å‡äº’åŠ¨ç‡
    avg_likes = total_likes // len(items_scraped) if items_scraped else 0
    avg_comments = total_comments // len(items_scraped) if items_scraped else 0

    logger.info(f"ğŸ“Š [{platform}] ç»Ÿè®¡:")
    logger.info(f"   - æ€»äº’åŠ¨: {total_likes:,} ç‚¹èµ, {total_comments:,} è¯„è®º")
    logger.info(f"   - å¹³å‡äº’åŠ¨: {avg_likes:,} ç‚¹èµ/æ¡, {avg_comments:,} è¯„è®º/æ¡")  # ğŸ†• æ–°åŠŸèƒ½
    logger.info(f"   - åª’ä½“ç±»å‹: {media_types_count}")

    return {
        "platform": platform,
        "platform_name": config["name"],
        "keywords": keywords,
        "items_scraped": len(items_scraped),
        "total_likes": total_likes,
        "total_comments": total_comments,
        "media_types": media_types_count,
        "data_file": str(data_file),
        "timestamp": datetime.datetime.now().isoformat(),
    }


async def crawler_loop():
    """æŒç»­è¿è¡Œçš„çˆ¬è™«ä¸»å¾ªç¯."""
    logger.info(f"ğŸ•·ï¸  [{PLATFORM}] çˆ¬è™«å¾ªç¯å¯åŠ¨...")
    logger.info(f"ğŸ“‹ [{PLATFORM}] é…ç½®:")
    logger.info(f"   - å¹³å°: {PLATFORM}")
    logger.info(f"   - å…³é”®è¯: {KEYWORDS}")
    logger.info(f"   - æœ€å¤§ç»“æœæ•°: {MAX_RESULTS}")
    logger.info(f"   - çˆ¬å–é—´éš”: {INTERVAL}ç§’")

    # åŠ è½½ä¹‹å‰çš„è¿›åº¦
    state = await load_progress()
    iteration = state.get("iteration", 0)

    try:
        while not shutdown_event.is_set():
            iteration += 1
            logger.info(f"ğŸ”„ [{PLATFORM}] ç¬¬ {iteration} æ¬¡çˆ¬å–...")

            # æ‰§è¡Œçˆ¬å–
            result = await crawl_platform(PLATFORM, KEYWORDS)

            # æ›´æ–°çŠ¶æ€
            import datetime
            state = {
                "platform": PLATFORM,
                "iteration": iteration,
                "last_update": datetime.datetime.now().isoformat(),
                "last_result": result,
            }

            # å®šæœŸä¿å­˜è¿›åº¦ï¼ˆæ¯æ¬¡çˆ¬å–åï¼‰
            await save_progress(state)

            # ç­‰å¾…ä¸‹ä¸€æ¬¡çˆ¬å–
            logger.info(f"â° [{PLATFORM}] ç­‰å¾… {INTERVAL} ç§’åç»§ç»­...")

            # å¯ä¸­æ–­çš„ç­‰å¾…
            try:
                await asyncio.wait_for(
                    shutdown_event.wait(),
                    timeout=INTERVAL
                )
                # å¦‚æœ shutdown_event è¢«è®¾ç½®ï¼Œé€€å‡ºå¾ªç¯
                if shutdown_event.is_set():
                    logger.info(f"ğŸ›‘ [{PLATFORM}] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
                    break
            except asyncio.TimeoutError:
                # ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡çˆ¬å–
                continue

    except asyncio.CancelledError:
        logger.info(f"âš ï¸  [{PLATFORM}] ä»»åŠ¡è¢«å–æ¶ˆ")
        raise
    except Exception as e:
        logger.error(f"âŒ [{PLATFORM}] çˆ¬è™«é”™è¯¯: {e}", exc_info=True)
        raise
    finally:
        # æ— è®ºå¦‚ä½•éƒ½ä¿å­˜è¿›åº¦
        await save_progress(state)


async def main():
    """Main entry point for crawler service."""
    global KEYWORDS  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGTERM, handle_shutdown)  # Docker stop
    signal.signal(signal.SIGINT, handle_shutdown)   # Ctrl+C

    logger.info(f"ğŸš€ Crawler service starting...")
    logger.info(f"ğŸ“‹ è¿›ç¨‹ ID: {os.getpid()}")
    logger.info(f"ğŸ·ï¸  å¹³å°: {PLATFORM}")
    logger.info(f"ğŸŒ ç¯å¢ƒ: {ENV}")

    # éªŒè¯é…ç½®
    if PLATFORM == "unknown":
        logger.error("âŒ æœªè®¾ç½® PLATFORM ç¯å¢ƒå˜é‡ï¼")
        logger.error("   è¯·åœ¨ docker-compose.yml ä¸­è®¾ç½® PLATFORM=x æˆ– PLATFORM=xhs")
        sys.exit(1)

    if not KEYWORDS or KEYWORDS == ['']:
        logger.warning("âš ï¸  æœªè®¾ç½® KEYWORDSï¼Œå°†ä½¿ç”¨é»˜è®¤å…³é”®è¯")
        KEYWORDS = ["default"]

    try:
        # è¿è¡Œçˆ¬è™«å¾ªç¯
        await crawler_loop()

    except KeyboardInterrupt:
        logger.info(f"âŒ¨ï¸  [{PLATFORM}] æ”¶åˆ°é”®ç›˜ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸ’¥ [{PLATFORM}] è‡´å‘½é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    finally:
        logger.info(f"ğŸ‘‹ [{PLATFORM}] Crawler service stopped gracefully")
        sys.exit(0)


if __name__ == "__main__":
    asyncio.run(main())
